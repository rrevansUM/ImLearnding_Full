---
title: Harville Chapter 1
author: Rich Evans
date: '2019-12-03'
slug: harville-chapter-1
categories:
  - Linear Algebra
  - Matrices
tags:
  - Linear Algebra
  - Matrices
type: ''
subtitle: ''
image: ''
---

# Intro

I've come across countless blogs ([Detroit data lab](https://detroitdatalab.com/), [Data Science +](https://datascienceplus.com/linear-algebra-in-r/), many others) attempting to cover the breadth of linear algebra using R, but each and every one of them peters out after some time. In this blog I would like to change that by exhausting myself, with the help of TV and others, to provide a complete treatment of Matrix Algebra for Statistics. Additionally, since many other blogs have tackled the basics, I will be skipping most of the theory and computation of the 1st chapter of Harville, opting instead to list a few important concepts to carry forward. Without further adieu,

# Harville, Chapter 1: Matrices

## Notation

Matrices are usually denoted 

$$
\textbf{A} = \{a_{ij}\} = \begin{pmatrix}
a_{11} & a_{12} & ... & a_{1n} \\
a_{21} & a_{22} & ... & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & ... & a_{mn}
\end{pmatrix}
$$

for $i = 1,...,m$ and $j = 1, ..., n$

## Basic Notation

Each $\textbf{bold}$ matrix will have lower case dimenions

Matrix | Dimensions
-------|--------------
A      | $m \times n$
B      | $p \times q$
C      | $s \times t$

* Things to remember
    + Commutativity
        + $A+B=B+A$ if $dim(A)=dim(B)$
    + Associativity
        + $A+(B+C)=(A+B)+C$ if $dim(A)=dim(B)=dim(C)$
    + Scalar Multiplication
        + $c(A+B)=cA+cB$
        + $(c+k)A=cA+kA$
        
We can show all of these properties real easy in R (or really any language)

```{r}
m <- 8
n <- 4

p <- m
q <- n

s <- m
t <- n

sampler <- function(...) {
  sample(1:10, 32, replace = TRUE)
}

A <- matrix(sampler(), nrow = m, ncol = n)
B <- matrix(sampler(), nrow = p, ncol = q)
C <- matrix(sampler(), nrow = s, ncol = t)
```

Commutativity,

```{r}
A
B
A + B
```

Associativity,

```{r}
A
B
C
all.equal(A + (B + C), (A + B) + C)
```

Scalar Multiplication,

```{r}
A
B
all.equal(2 * (A + B), 2 * A + 2 * B)
```

These results are all extremely boring, it's more fun when they fail,

```{r}
D <- matrix(sample(1:10, 4, replace = TRUE), nrow = 2, ncol = 2)

tryCatch(A + D, error = function(e) e)
```

Still not all that interesting.

## Matrix Multiplication

Let's change B's dimensions so that it can still conform with A (m=8, n=4)

```{r}
B <- matrix(sample(1:10, 16, replace = TRUE), nrow = 4, ncol = 8)
```

So the dimensions are not equal,

```{r}
A %*% B
```

An $8 \times 8$ matrix. It's important in linear algebra two pay attention to both the dimensional calculus and the matrix arithmetic, always keeping in mind the number of rows and columns involved. 

Each element of the matrix product $AB$ is just a linear sum of the columns of $A$ by the rows of $B$, 

$$
\sum_{k=1}^na_{ik}b_{kj} = a_{i1}b_{1j} + a_{i2}b_{2j} + \dots + a_{in}b_{nj}
$$

So the $ij$th element, 222 in $AB$ is $(6 \times 8) + (9 \times 10) + (8 \times 7) + (4 \times 7)$ ... pretty simple.

Harville makes a distinction between *premultiplication* and *postmultiplication* for the matrix product $AB$, the former being "$B$ times $A$" and the later being "$A$ times $B$". I hardly think this matters but, maybe I'll be surprised later on in the chapter.

* Matrix Multiplication Properties
    + Matrix Multiplication **is Associative**
        + $A(BC) = (AB)C$ as long as $C$ is $q \times  t$
    + Matrix Multiplication **is Distributive with respect to Addition**
        + $A(B + C) = AB + AC$
    + Matrix Multiplication **is not Commutative**
        + in the general case, $AB \ne BA$
    + Scalar Multiplication **is a thing**
        + $cA_1A_2 \cdots A_k = (cA_1)A_2 \cdots A_k = A_1(cA_2) \cdots A_k = A_1A_2 \cdots (cA_k)$

Commutativity of Matrix Multiplication should be explained a bit more in depth,

```{r}
A %*% B
B %*% A
```

Clearly these are very different matrices. Often matrix multiplication goes undefined for example, it is always the case that, for matrices $\underset{m \times n}{A}$ and $\underset{p \times n}{B}$ $AB$ is defined but $BA$ is not defined.

```{r}
B <- matrix(sample(1:10, size = 16, replace = TRUE), nrow = 4, ncol = 4)

A %*% B # defined
tryCatch(B %*% A, error = function(e) e)
```

Some square matrices **can** commute though, and collections of matrices can commute but only in pairs, 

$$A_iA_j = A_jA_i \quad for \quad j > i = 1, 2, \dots$$

Which may be important later.

## Transposition

transposing $\underset{m \times n}{A}$ basically rotates it, transforming it into $\underset{n \times m}{A'}$

```{r}
A
t(A)
```

But matrix tranposes are like negations, negate a negation and you get the original thing,

```{r}
all.equal(t(t(A)), A)
```

If $A+B$ is defined then $(A+B)'=A'+B'$.

```{r}
B <- matrix(sampler(), nrow = p, ncol = q)

all.equal(t(A + B), t(A) + t(B))
```

Likewise, if $AB$ is defined then $(AB)'=B'A'$

```{r}
B <- matrix(sample(1:10, 16, replace = TRUE), nrow = 4, ncol = 4)

all.equal(t(A %*% B), t(B) %*% t(A))
```

So that's weird right? Why must the product flip?

```{r}
t(A %*% B)
t(B) %*% t(A)
```

The $ij$th element of $(AB)'$ is the $ji$th element of $AB$

```{r}
t(A %*% B)
A %*% B
```

## Matrix Types

### Square Matrices

We've already seen these,

```{r}
D
```

The only interesting property of square matrices is that $DD$ is defined, or for any number of products $k$, $D^k=DD^{k-1}=DDD^{k-2}$

```{r}
D %*% D %*% D %*% D
```

### Symmetric Matrices

if $A=A'$ then it's symmetric,

```{r}
isSymmetric(A)
```

### Diagonal Matrices

$$
\mathbf{D} =
  \begin{pmatrix}
    d_{11} & 0 & \cdots & 0 \\
    0 & d_{22} & \cdots & 0 \\
    \vdots & & \ddots & \\
    0 & 0 & \cdots & d_{nn}
  \end{pmatrix}
$$

Or sometimes just,

$$
\mathbf{D} =
  \begin{pmatrix}
    d_1 & 0 & \cdots & 0 \\
    0 & d_2 & \cdots & 0 \\
    \vdots & & \ddots & \\
    0 & 0 & \cdots & d_n
  \end{pmatrix} = diag(d_1, d_2, \dots, d_n)
$$

For $\underset{m \times n}{A}$ and $diag(d_1, d_2, \dots, d_m)$, $DA = d_ia_{ij}$ 

```{r}
D <- diag(1:8, nrow = 8, ncol = 8)
D
D %*% A
```

#### Identity Matrices

Identity matrices are diagonal matrices and thus, are only square

$$
\mathbf{I_n} =
  \begin{pmatrix}
    1 & 0 & \cdots & 0 \\
    0 & 1 & \cdots & 0 \\
    \vdots & & \ddots & \\
    0 & 0 & \cdots & 1
  \end{pmatrix}
  = diag(1, 1, \dots, 1)
$$

$IA=AI=A$ is always true when the dimensions are amenable.

```{r}
I <- diag(1, 4, 4)
all.equal(I %*% B, B %*% I, B)
```

### Matrices of 1s

$$
\mathbf{J_{mn}} =
  \begin{pmatrix}
    1 & 1 & \cdots & 1 \\
    1 & 1 & \cdots & 1 \\
    \vdots & \vdots & \ddots & \vdots \\
    1 & 1 & \cdots & 1
  \end{pmatrix}
  = J
$$

Harville makes note of the result $J_{mn}J_{np}=nJ_{mp}$, which makes a lot of intuitive sense.

```{r}
m <- 4
n <- 5
p <- 3
J_mn <- matrix(rep(1, 20), nrow = m, ncol = n)
J_np <- matrix(rep(1, 15), nrow = n, ncol = p)
(J_mn %*% J_np) / n
```

### Null Matrices

$$
\mathbf{\textbf{0}} =
  \begin{pmatrix}
    0 & 0 & \cdots & 0 \\
    0 & 0 & \cdots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \cdots & 0
  \end{pmatrix}
$$

You can probably guess that null matrices have intuitive properties and you'd be right,

$\textbf{0A}=\textbf{A0}=\textbf{0}$ if possible

```{r}
nullMat <- matrix(rep(0, 16), nrow = m, ncol = m)
B %*% nullMat
nullMat %*% B
```

For some scalar $k$, $k\textbf{0}=\textbf{0}$

$$
\textbf{A} + \textbf{0}=\textbf{0} + \textbf{A} = \textbf{A} \\
\textbf{A} - \textbf{A} = \textbf{0}
$$

### Triangular Matrices

Triangular matrices must be square,

$$
\mathbf{\textbf{T}} =
  \begin{pmatrix}
    t_{11} & t_{12} & \cdots & t_{1n} \\
    0 & t_{22} & \cdots & t_{2n} \\
    \vdots & & \ddots & \\
    0 & 0 & \cdots & t_{nn}
  \end{pmatrix}
$$

If a matrix is both **upper triangular** and **lower triangular**, then it's just a diagonal matrix. If you tranpose a triangular matrix it flips from lower (upper) to an upper (lower) triangular matrix,

$$
\mathbf{\textbf{T}'} =
  \begin{pmatrix}
    t_{11} & 0 & \cdots & 0 \\
    t_{12} & t_{22} & \cdots & 0 \\
    \vdots & \vdots & \ddots &\vdots  \\
    t_{1n} & t_{2n} & \cdots & t_{nn}
  \end{pmatrix}
$$

```{r}
T.tri <- B
T.tri[upper.tri(T.tri)] <- 0
T.tri
```

```{r}
T.tri + T.tri
```

Still lower triangular. Now for some important things to remember, at least when it comes to statistical mathematics, first **triangular matrix multiplication**,

For two matrices $\underset{n \times n}{\textbf{A}}, \underset{n \times n}{\textbf{B}}$ that are both **upper triangular** (could be **lower triangular**), their premultiplication $AB$ would be upper triangular,

```{r}
T.tri %*% T.tri
```

There's a proof in the chapter for a result similar to this but theres really no merit in including it here.

**Def.** *unit triangular matrices* - a triangular matrix where the diagonal elements are all 1's.

$$
\mathbf{\textbf{U}} =
  \begin{pmatrix}
    1 & 0 & \cdots & 0 \\
    t_{12} & 1 & \cdots & 0 \\
    \vdots & \vdots & \ddots &\vdots  \\
    t_{1n} & t_{2n} & \cdots & 1
  \end{pmatrix}
$$

### Row/Column Vectors

These are self-explanatory,

A row vector

```{r}
a <- matrix(1:5, nrow = 1, ncol = 5)
a
```

A column vector

```{r}
t(a)
```

Common notation for **identity row (column) vectors** look like $\textbf{1}_m$ or $\textbf{1}_n$. But check this out,

```{r}
onesM <- matrix(rep(1, m), nrow = m, ncol = 1)
onesN <- matrix(rep(1, n), nrow = 1, ncol = n)
onesM %*% onesN
```

which is $\textbf{1}_m \textbf{1}_n' = \textbf{J}_{mn}$

### 1x1 matrices

Basically, there could be $\underset{1 \times 1}{\textbf{k}}$ and $k$, both are pretty much just a scalar but the reader should be aware that it may show up in some results.

## Exercises

For most of these I can cheat and use R, down the line I'll attempt to prove things on my own.

### 1.1

**Prove**: $(A+B)+C=A+(B+C)$

I demonstrated this one above.

### 1.2

Let $A$ represent an $m \times n$ matrix and $B$ an $n \times p$ matrix, and let $c$ and $k$ represent arbitrary scalars. Using results (2.9) and (2.1) (or other means), show that

$$(cA)(kB) = ck(AB)$$

```{r}
c <- 2
k <- 10

all.equal((c * A) %*% (k * B), (c * k) * (A %*% B))
```

### 1.3

* (a) Verify result (2.6) (on the associativeness of matrix multiplication); that is, show that, for any $m \times n$ matrix $A = \left\{ a_{ij} \right\}$, $n \times q$ matrix $B = \left\{ b_{jk} \right\}$, and $q \times r$ matrix $C = \left\{ c_{ks} \right\}$, $A(BC) = (AB)C$.
* (b) Verify result (2.7) (on the distributiveness with respect to addition of matrix multiplication); that is, show that, for any $m \times n$ matrix $A = \left\{ a_{ij} \right\}$ and $n×q$ matrices $B = \left\{b_{jk}\right\}$ and $C=\left\{c_{jk}\right\}$, $A(B + C) = AB + AC$.

I showed these above, with slightly different dimensions, but I think Harville was looking for more of a dimensional analysis like so, for part a,

$$
\begin{align}
\sum_j a_{ij} \left( \sum_k b_{jk}c_{ks} \right) & = \sum_j \left( \sum_k a_{ij}b_{jk}c_{ks} \right) \\
& = \sum_k \left(\sum_j a_{ij}b_{jk}c_{ks} \right) \\
& = \sum_k \left(\sum_ja_{ij}b_{jk} \right)c_{ks} \\
& = \left(\sum_ja_{ij}b_{jk} \right)\sum_kc_{ks}
\end{align}
$$

Which should be pretty obvious. Part (b) will be pretty much the same kind of thing.

### 1.4

Let $A=\left\{a_{ij}\right\}$ represent an $m×n$ matrix and $B=\left\{b_{ij}\right\}$ a $p×m$ matrix. 

Our matrix $\textbf{A}$ stored in `A` is `r dim(A)` (m = 8, n = 4). We'll set $\textbf{B}$ to be be $p \times m$

```{r}
p <- 3
m <- 8
B <- matrix(sample(1:10, 24, replace = T), nrow = p, ncol = m)
```


(a.) Let $\textbf{x}=\left\{x_i\right\}$ represent an $n$-dimensional column vector. Show that the $i$th element of the $p$-dimensional column vector $\textbf{BAx}$ is

$$\sum_{j=1}^mb_{ij} \sum_{k=1}^na_{jk}x_k$$

```{r}
n <- ncol(A)
x <- matrix(sample(1:10, n, replace = T), nrow = n, ncol = 1)
```

So, what would $\textbf{Ax}$ look like,

```{r}
A %*% x
```

The $jk$th element is the sum across the columns of $\textbf{A}$ and the rows of $\textbf{x}$, or similarly, the $k$th row.

```{r}
B %*% (A %*% x)
```

As $\textbf{B}$ has $p$ rows (in our case, 3), $\textbf{BAx}$ is a $p$-dimensional column vector.

parts (b)-(d) can be "solved" similarly and they're not particularly interesting, so I will skip their demonstrations/solutions.

(b.) Let $\textbf{X} = \left\{x_{ij}}\right\}$ represent an $n × q$ matrix. Generalize formula (E.1) by expressing the $ir$th element of the $p × q$ matrix $\textbf{BAX}$ in terms of the elements of $\textbf{A}$, $\textbf{B}$, and $\textbf{X}$. 

(c.) Let $\textbf{x} = \left\{x_i\right\}$ represent an $n$-dimensional column vector and $C=\left\{c_{ij}\right\}$ a $q × p$ matrix. Generalize formula (E.1) by expressing the ith element of the q-dimensional column vector $\textbf{CBAx}$ in terms of the elements of $\textbf{A}$, $\textbf{B}$, $\textbf{C}$, and $\textbf{X}$.  

(d.) Let $y=\left\{y_i\right\}$ represent a $p$-dimensional column vector. Express the $i$th element of the $n$-dimensional row vector $y=BA$ in terms of the elements of $\textbf{A}$, $\textbf{B}$, and $\textbf{y}$.

### 1.5

Let $\textbf{A}$ and $\textbf{B}$ both be $m \times n$ matrices, show that

$$
(\textbf{A} + \textbf{B})(\textbf{A}-\textbf{B}) = \textbf{A}^2-\textbf{B}^2
$$

Given that $\textbf{A}$ and $\textbf{B}$ commute. I think he means $n \times n$ matrices, as this would only make sense for $m=n$ scenarios.

```{r}
n <- 4
A <- matrix(1:16, nrow = n, ncol = n)
B <- matrix(1:16, nrow = n, ncol = n)
```

If they commute then $\textbf{A}\textbf{B} = \textbf{B}\textbf{A}$. They would **have** to be square, they may not have to be identical.

Because of the distributive-ness of matrix multiplication, 

$$
\textbf{A}\textbf{A}-\textbf{A}\textbf{B}+\textbf{B}\textbf{A}-\textbf{B}\textbf{B} = \textbf{A}^2-\textbf{A}\textbf{B}+\textbf{B}\textbf{A} - \textbf{B}^2
$$

Which cannot be $\textbf{A}^2-\textbf{B}^2$ unless $\textbf{A}\textbf{B} = \textbf{B}\textbf{A}$ cuz then the middle term will cancel out. $\textbf{Q.E.D.}$ broh!

Also,

```{r}
A %*% B - B %*% A
```

### 1.6

Show that the product $\textbf{AB}$ of two $n×n$ symmetric matrices $\textbf{A}$ and $\textbf{B}$ is itself symmetric *iff* $\textbf{A}$ and $\textbf{B}$ commute.

```{r}
A <- Matrix::forceSymmetric(A)
B <- Matrix::forceSymmetric(B)

A
```



# Appendix

## Session Info

```{r, echo = FALSE, eval = TRUE}
sessionInfo()
```
